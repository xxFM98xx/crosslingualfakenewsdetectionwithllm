
---

### 2. **README.md model_training**

```markdown
# Modelltraining

This directory contains code and scripts for training classifiers as part of the master's thesis **"Cross-language Fake News Detection using Large Language Models"**.

## Inhalt

- **grid_search.py**: Implements a grid search for hyperparameter optimization and for training the classifiers.

- **main.py**: Main entry point for model training.

- **requirements_models.txt**: Dependencies or packages for model training.

### Modelle

- **models/arg.py**: Defines the ARG network architecture, trainers and test/predict functions.

- **models/baseline.py**: Contains architectures of the comparison classifiers, as described in the thesis, as well as trainer, predict and test functions.

- **models/layers.py**: Contains user-defined layers that are used in the ARG network.

- **models/argd.py**: **Not used in this work.*

### Utils

- **utils/dataloader.py**: Functions for data preprocessing for the classifiers to make the data accessible for the classifiers.

- **utils/utils.py**:  Helper functions for model training and logging.

### Skripte

- **scripts/run_falcon_7b_arg_chinese_source_RoBERTa_CNN_MLP_rationales_True.sh**: Shell script for training classifiers on the rationales generated by the Falcon 7B LLM with associated data for the ARG Chinese dataset.

### SLM

- **slm/**: Contains the SLM model (e.g. XLM-Roberta).

## Instructions for training the classifiers

### Prerequisites

- Python 3.10.5
- Installation of the required packages with `requirements_models.txt`.

### Steps

#### 1. Set up the environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements_models.txt
```
#### 2. Download SLM
The SLM must be downloaded and saved in the `slm` folder. https://huggingface.co/FacebookAI/xlm-roberta-base](https://huggingface.co/FacebookAI/xlm-roberta-base) was used in this work.

#### 3. Execution of scripts in `model_training/scripts`.

In `scripts` a `.sh` script can be found, which was built for the Falcon 7B LLM on the ARG Chinese dataset with the classifier RoBERTa_CNN_MLP. To build the experiment, the script for each LLM must be `[tiiuae/falcon-7b-instruct, FreedomIntelligence/phoenix-inst-chat-7b, DAMO-NLP-MT/polylm-chat-13b, Qwen/Qwen2-7B-Instruct, google/gemma-1.1-7b-it, meta-llama/Meta-Llama-3. 1-8B-Instruct, meta-llama/Llama-2-7b-chat-hf, SeaLLMs/SeaLLMs-v3-7B-Chat]`, each data set from the list `[ARG-ENGLISH, ARG-CHINESE, BanFakeNews, FANG-COVID, AFND]` and each classifier from the list `['RoBERTa_MLP', 'RoBERTa_CNN_MLP', 'ARG']`. The `.sh` file explains which changes have to be made and how in order to train each combination of LLM x data set x variant x classifier.

#### 4. After training a classifier

After training a classifier, a subdirectory `log` is created in the `model_training/scripts` directory, which provides the performance of the classifier based on `DATA_NAME` (defined in the `.sh` script).

#### 5. Transfer classifier performances to dataframes

To transfer the classifier performances to data frames, as shown in the tables in the thesis, `Evaluator.all_classifier_evaluation_into_several_csv()` can be used. To do this, these must first be used with the help function `read_files_into_list_of_dicts(“path/to/logs”)` (see `utils/__init__.py`), which converts the logs into a list of dicts. The list of dicts can then be passed to `Evaluator.all_classifier_evaluation_into_several_csv()` to create the dataframes or tables.

