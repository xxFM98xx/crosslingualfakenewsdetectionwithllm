import os
import pandas as pd
from classes.TrainingDataPrepperModule import DataPrepper
from classes.Evaluator import Evaluator
import json
import re
from collections import defaultdict
import numpy as np
from utils import read_files_into_list_of_dicts_json, read_files_into_list_of_dicts_csv, combine_json_files_in_subdir


# SPECIFY THE PATH TO THE DIRECTORY CONTAINING THE LLM REPORT JSON FILES FOR EXAMPLE LIKE BELOW. NEEDED FOR RQ2 TO EVALUATE THE CROSS DATASET PERFORMANCE OF THE LLM MODELS
LLM_REPORT_PATH = r"\cross-lingual-fake-news-detection-with-llm\Reports"
# JUST A REFERENCE  WHERE THE CROSS DS EVALUATION CSV FILE WILL BE SAVED BY THE FUNCTION cross_ds_evaluation OF THE EVALUATOR CLASS CALLED IN THE FUNCTION all_arg_llm_cross_ds_ OF THIS SCRIPT
LLM_CROSS_DS_PATH = r"C:\Users\FilmonMesfun\Desktop\MT_FM\cross-lingual-fake-news-detection-with-llm\Reports\cross_ds_evaluation.csv"


# Create the directory if it does not exist
os.makedirs(LLM_REPORT_PATH, exist_ok=True)



def all_arg_llm_cross_ds_(LLM_REPORT_PATH:str):
    """
    Evaluates the cross-dataset performance of the LLM models by reading the reports from LLM_REPORT_PATH.(The reports in LLM_REPORT_PATH should be generated by Evaluator class calling the evaluate_llm method).
    It then saves the results in LLM_CROSS_DS_PATH as a CSV file containing the cross-dataset evaluation results.
    This function utilizes the Evaluator class to perform the evaluation by calling the cross_ds_evaluation method.
    
    Args:
        LLM_REPORT_PATH (str): The path to the directory containing the LLM reports.
        
    Returns:
        None
    """
    evaluator = Evaluator(ground_truth_column_name='label', predicted_column_names=['td_pred', 'cs_pred'])
    #get all filenames in LLM_REPORT_PATH which start with all__llm_reports_{whatever}_cleaned.csv
    print(f"Searching for files in {LLM_REPORT_PATH} which contains {os.listdir(LLM_REPORT_PATH)}")
    llm_files_single_ds_report = [file for file in os.listdir(LLM_REPORT_PATH) if file.startswith("all_llm_reports_") and file.endswith("_cleaned.csv")]

    print(f"Found {len(llm_files_single_ds_report)} files in {LLM_REPORT_PATH}")
    single_ds_report = {}

    #get all csv files into df list filenames dict
    for filename in llm_files_single_ds_report:
        df = pd.read_csv(os.path.join(LLM_REPORT_PATH, filename))
        single_ds_report[filename] = df

    # get into evaluator to key_metrices and do the evaluation cross ds
    evaluator.cross_ds_evaluation(single_ds_report,LLM_REPORT_PATH)



# Function to extract metric values, ignoring standard deviations or percentages
# Preparation for RQ2
def extract_value(metric):
    """
    Extracts the primary value from a metric string, ignoring standard deviations or percentages.
    
    Args:
        metric (str): The metric value to extract.
    """
    if isinstance(metric, str):
        # Remove "±" or "%" and extract the primary value
        if "±" in metric:
            return float(metric.split("±")[0].strip())
        if "%" in metric:
            return None  # Ignore percentage metrics
        if metric.strip() == "-":
            return None  # Treat "-" as missing
    try:
        return float(metric)
    except (ValueError, TypeError):
        return None

# Function to filter out rows with 'improvement' or 'difference' in 'modelname_source'
# Preparation for RQ2
def filter_invalid_rows(original_df):
    """
    Filter out rows with 'improvement' or 'difference' in 'modelname_source'.
    
    Args:
        original_df (pd.DataFrame): The original DataFrame to filter.
        
    Returns:
        pd.DataFrame: The filtered DataFrame.
    """    

    return original_df[
        ~original_df["modelname_source"].str.contains("improvement|difference", case=False, na=False)
    ]

# Function to process the original DataFrame into the correct structure
# Preparation for RQ2
def process_original_dataframe(original_df):
    """
    Process the original DataFrame into the correct structure for RQ2.
    
    Args:
        original_df (pd.DataFrame): The original DataFrame to process.(Generated by cross_ds_evaluation function of Evaluator)
    
    Returns:
        pd.DataFrame: The processed DataFrame.
    """
      
    # Filter out invalid rows
    filtered_df = filter_invalid_rows(original_df)

    # Initialize the list for transformed data
    transformed_data = []

    # Map datasets to their languages
    dataset_language_dict = {
        "arg_chinese": "zh",
        "arg": "en",
        "asdn": "ar",
        "ban": "bn",
        "fang": "de",
    }

    # Define valid perspectives
    valid_perspectives = ["cs", "td"]

    # Iterate over rows in the filtered DataFrame
    for _, row in filtered_df.iterrows():
        model_info = row["modelname_source"]
        model_name_parts = model_info.split("_")
        model = model_name_parts[0]  # Extract the model name
        source_type = model_name_parts[1] if len(model_name_parts) > 1 else None

        if source_type not in ["llm", "source", "google"]:
            continue  # Skip irrelevant rows

        # Process each column for weighted F1 metrics
        for col in filtered_df.columns[1:]:
            if "_pred_weighted_f1" not in col:
                continue  # Process only weighted F1 columns

            parts = col.split("_")
            if not "arg" in parts[0] and not "chinese" in parts[1]:
                dataset_name = parts[0]
                perspective = parts[1]  # Extract perspective
            else:
                if "chinese" in parts[1] and "arg" in parts[0]: 
                    dataset_name = "arg_chinese"
                    perspective = parts[2]
                elif "arg" in parts[0]:
                    dataset_name = "arg"
                    perspective = parts[1]

            if dataset_name not in dataset_language_dict:
                continue
            if perspective not in valid_perspectives:
                continue  # Skip invalid perspectives

            language = dataset_language_dict[dataset_name]
            metric_value = extract_value(row[col])

            if metric_value is not None:
                transformed_data.append({
                    "Model": model,
                    "Language": language,
                    "Perspective": perspective,
                    "Technik": source_type,
                    "F1": metric_value,
                })

    # Convert the transformed data into a DataFrame
    transformed_df = pd.DataFrame(transformed_data)

    return transformed_df

# Function to create the language distribution DataFrame combined with the F1 metrics
# Preparation for RQ2
def create_distribution_df(transformed_df):
    """
    Create the distribution DataFrame for RQ2.
    
    Args:
        transformed_df (pd.DataFrame): The transformed DataFrame to process.(Generated by process_original_dataframe function)
    
    Returns:
        pd.DataFrame: The distribution DataFrame.
    """
    # Pivot the table to have techniques as columns
    pivoted_df = transformed_df.pivot_table(
        index=["Model", "Language", "Perspective"],
        columns="Technik",
        values="F1",
        aggfunc="first"
    ).reset_index()

    # Rename technique columns
    pivoted_df.rename(columns={
        "google": "F1 (Google)",
        "llm": "F1 (LLM)",
        "source": "F1 (Source)"
    }, inplace=True)

    # Calculate differences
    pivoted_df["Δ F1 (LLM - Source language)"] = pivoted_df["F1 (LLM)"] - pivoted_df["F1 (Source)"]
    pivoted_df["Δ F1 (Google - Source language)"] = pivoted_df["F1 (Google)"] - pivoted_df["F1 (Source)"]

    #Round the values to 2 decimal places of the F1 differences
    pivoted_df["Δ F1 (LLM - Source language)"] = pivoted_df["Δ F1 (LLM - Source language)"].round(2)
    pivoted_df["Δ F1 (Google - Source language)"] = pivoted_df["Δ F1 (Google - Source language)"].round(2)

    # Add language distribution data
    model_name_language_distribution = {
        "llama2": [89.70, 0.17, 0.13,0, 0 ],
        "polylm": [67.56, 0.88, 22.14, 0.55, 0],
        "phoenix": [58.6, 0.9, 20.9, 0.8, 0.5],
    }

    language_columns = ["en", "de", "zh", "ar", "bn"]

    for idx, row in pivoted_df.iterrows():
        model = row["Model"]
        distribution = model_name_language_distribution.get(model, [None] * 5)
        for lang, value in zip(language_columns, distribution):
            pivoted_df.at[idx, lang] = value if value is not None else "-"
    
   # remove rows that contain models that are not in model_name_language_distribution
   pivoted_df = pivoted_df[pivoted_df["Model"].isin(model_name_language_distribution.keys())]
   # remove rows that contain the model falcon with the language ar
   pivoted_df = pivoted_df[~((pivoted_df["Model"] == "falcon") & (pivoted_df["Language"] == "ar"))]
   # remove rows that contain the model polylm with the language de
   pivoted_df = pivoted_df[~((pivoted_df["Model"] == "polylm") & (pivoted_df["Language"] == "de"))]
   # remove rows that contain the model phoenix with the language en
   pivoted_df = pivoted_df[~((pivoted_df["Model"] == "phoenix") & (pivoted_df["Language"] == "en"))]
   # replace the value in the column F1 (English) and Δ F1 (English - Source language) with "-" if the language is en
    pivoted_df.loc[pivoted_df["Language"] == "en", ["F1 (LLM)","F1 (Google)", "Δ F1 (LLM - Source language)", "Δ F1 (Google - Source language)"]] = "-" 
    #all nan values in the df columns are replaced with "-"
    pivoted_df = pivoted_df.fillna("-")
    pivoted_df = pivoted_df.sort_values(by=["Model", "Language"]).reset_index(drop=True)

    return pivoted_df

# Function to create the binary languages DataFrame. Binary languages are represented as 1 if the language is supported by the model, and 0 otherwise which just shows the supported languages of the models
## Preparation for RQ2
def create_binary_languages_df(distribution_df):
    """
    Create the binary languages DataFrame for RQ2.
    Binary languages are represented as 1 if the language is supported by the model, and 0 otherwise.
    
    Args:
        distribution_df (pd.DataFrame): The distribution DataFrame to process. (Generated by create_distribution_df function)
    
    Returns:
        pd.DataFrame: The binary languages DataFrame.
    """
    model_name_language_supported = { 
        "llama2": ['en'],
        "llama3.1": ['en', 'de'],
        "polylm": ['ar', 'zh', 'en', 'de'],
        "qwen2": ['ar', 'bn', 'zh', 'en', 'de'],
        "seallm3": ['ar', 'bn', 'zh', 'en', 'de'],
        "phoenix": ['ar', 'zh', 'en', 'de'],
        "gemma1.1": ['en'],
        "falcon": ['en']
    }
    binary_languages_data = []
    

    for _, row in distribution_df.iterrows():
        model = row["Model"]
        language = row["Language"]
        perspective = row["Perspective"]

        binary_row = {
            "Model": model,
            "Language": language,
            "Perspective": perspective,
            "F1 (Google)": row["F1 (Google)"],
            "F1 (LLM)": row["F1 (LLM)"],
            "F1 (Source)": row["F1 (Source)"],
            "Δ F1 (LLM - Source language)": row["Δ F1 (LLM - Source language)"],
            "Δ F1 (Google - Source language)": row["Δ F1 (Google - Source language)"],
            "en": 1 if "en" in model_name_language_supported.get(model, []) else 0,
            "de": 1 if "de" in model_name_language_supported.get(model, []) else 0,
            "zh": 1 if "zh" in model_name_language_supported.get(model, []) else 0,
            "ar": 1 if "ar" in model_name_language_supported.get(model, []) else 0,
            "bn": 1 if "bn" in model_name_language_supported.get(model, []) else 0,
        }
        binary_languages_data.append(binary_row)

    binary_languages_df = pd.DataFrame(binary_languages_data)
    # delete rows that contain models that are not in model_name_language_supported
    binary_languages_df = binary_languages_df[binary_languages_df["Model"].isin(model_name_language_supported.keys())]
    # replace the value in the column F1 (English) and Δ F1 (English - Source language) with "-" if the language is en
    binary_languages_df.loc[binary_languages_df["Language"] == "en", ["F1 (Englisch)", "Δ F1 (Englisch - Source language)"]] = "-"
    #all nan values in the df columns are replaced with "-"
    binary_languages_df = binary_languages_df.fillna("-")

    binary_languages_df = binary_languages_df.sort_values(by=["Model", "Language"]).reset_index(drop=True)

    return binary_languages_df

# Preparation for RQ2
def transform_llm_into_la_info_supported(df_original):
    """
    Transform the original DataFrame into the language information and supported languages DataFrames.
    
    Args:
        df_original (pd.DataFrame): The original DataFrame to process.
    
    Returns:
        pd.DataFrame: The language information DataFrame.
        pd.DataFrame: The supported languages DataFrame.
    """
    
    model_name_language_distribution = {
        "llama2": [89.70, 0.17, 0.13,0, 0 ],
        "polylm": [67.56, 0.88, 22.14, 0.55, 0],
        "phoenix": [58.6, 0.9, 20.9, 0.8, 0.5],
    }
    # Process the original data
    transformed_df = process_original_dataframe(df_original)

    # Create the distribution DataFrame
    distribution_df = create_distribution_df(transformed_df)

    # Create the binary languages DataFrame
    binary_languages_df = create_binary_languages_df(distribution_df)
    # remove rows that contain models that are not in model_name_language_distribution
    distribution_df = distribution_df[distribution_df["Model"].isin(model_name_language_distribution.keys())]
    # remove rows that contain the model falcon with the language ar
    distribution_df = distribution_df[~((distribution_df["Model"] == "falcon") & (distribution_df["Language"] == "ar"))]
    # remove rows that contain the model polylm with the language de
    distribution_df = distribution_df[~((distribution_df["Model"] == "polylm") & (distribution_df["Language"] == "de"))]
    # remove rows that contain the model phoenix with the language en
    distribution_df = distribution_df[~((distribution_df["Model"] == "phoenix") & (distribution_df["Language"] == "en"))]
    distribution_df = distribution_df.fillna("-")

    return distribution_df, binary_languages_df


        




#NEEDS TO BE CALLED FOR RQ2
#all_arg_llm_cross_ds_(LLM_REPORT_PATH)
#data_df, binary_data_df = transform_llm_into_la_info_supported(pd.read_csv(LLM_CROSS_DS_PATH))











